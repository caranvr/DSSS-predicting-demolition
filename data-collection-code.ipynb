{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting building demolition risk in Philadelphia's residential neighbourhoods, 2018-2021\n",
    "\n",
    "#### Demolitions and property characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data on private demolitions post-2018 from API\n",
    "demolitions = pd.read_csv(\"https://phl.carto.com/api/v2/sql?format=CSV&q=SELECT%20address,%20typeofwork%20FROM%20demolitions%20WHERE%20city_demo%20=%20%27NO%27%20AND%20start_date%20>=%20%272018-01-01%27\")\n",
    "\n",
    "demolitions.drop(index=demolitions[demolitions['typeofwork'] == 'TANKRI'].index, inplace=True) #drop tank removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get selected features of all properties in Philadelphia\n",
    "chunks = []\n",
    "dtypes = {\n",
    "    'lng': 'float64',\n",
    "    'lat': 'float64',\n",
    "    'location': 'object',\n",
    "    'category_code_description': 'object',\n",
    "    'interior_condition': 'object', #ordinal scale, therefore encoding this feature as a category\n",
    "    'exterior_condition': 'object',\n",
    "    'total_area': 'float64',\n",
    "    'year_built': 'object',\n",
    "    'parcel_number': 'object'\n",
    "}\n",
    "\n",
    "chunked_df = pd.read_csv('https://phl.carto.com/api/v2/sql?format=CSV&q=SELECT%20ST_X(the_geom)%20AS%20lng,%20ST_Y(the_geom)%20AS%20lat,%20location,%20category_code_description,%20interior_condition,%20exterior_condition,%20total_area,%20year_built,%20parcel_number%20FROM%20opa_properties_public', \n",
    "                         dtype=dtypes,\n",
    "                         chunksize=40000)\n",
    "\n",
    "for chunk in chunked_df:\n",
    "    chunk['demolition'] = chunk['location'].isin(demolitions['address']).astype(np.int8) #create binary field encoding whether or not an address is associated with a demolition permit\n",
    "    chunks.append(chunk)\n",
    "\n",
    "properties = pd.concat(chunks)\n",
    "properties.drop(index=properties[properties['category_code_description'] == 'Vacant Land'].index, inplace=True)\n",
    "\n",
    "#Load in market value for properties in 2018\n",
    "value18 = pd.read_csv('https://phl.carto.com/api/v2/sql?format=CSV&q=SELECT%20parcel_number,%20market_value%20FROM%20assessments%20WHERE%20year%20=%202018',\n",
    "                     dtype={\n",
    "                         'parcel_number': 'object',\n",
    "                         'market_value': 'float64'\n",
    "                     })\n",
    "\n",
    "value18.loc[\n",
    "    value18['market_value'] == 0\n",
    "]\n",
    "\n",
    "properties = pd.merge(properties, value18, how='left', on='parcel_number')\n",
    "\n",
    "#Drop properties with text or null values in year_built field\n",
    "properties.drop(index=properties[properties['year_built'].str.contains(r'[A-Za-z]+', na=True)].index, inplace=True)\n",
    "properties['year_built'] = properties['year_built'].astype('int64')\n",
    "\n",
    "#Drop properties built during or after 2018\n",
    "properties.drop(index=properties[properties['year_built'] >= 2018].index, inplace=True)\n",
    "\n",
    "del(value18) #save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop properties with null lat and long coordinates (they show up in pandas.info() as null, but not in the preview,\n",
    "#but they still cause bugs in spatial operations)\n",
    "properties.drop(index=properties['lat'][properties['lat'].isna()].index, inplace=True)\n",
    "\n",
    "#Convert data frame to georeferenced dataframe to \n",
    "geo_properties = gpd.GeoDataFrame(properties,\n",
    "                                  geometry=gpd.points_from_xy(properties.lng, properties.lat))\n",
    "\n",
    "geo_properties.set_crs('epsg:4326', inplace=True) #set projection to WGS84\n",
    "geo_properties.to_crs('epsg:2272', inplace=True) #reproject to NAD 1983 for southern PA\n",
    "geo_properties.set_index('parcel_number', inplace=True) #set index\n",
    "\n",
    "del(properties) #save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance to City Hall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    POINT (2693536.305 236112.283)\n",
       "dtype: geometry"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Easting and Northing coordinates for City Hall\n",
    "city_hall = gpd.GeoSeries(Point(-75.1635112, 39.952335), crs=4326)\n",
    "city_hall.to_crs('epsg:2272') #reproject to NAD 1983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn City Hall into point object\n",
    "city_hall = Point(2693536.305, 236112.283)\n",
    "\n",
    "#Add field with distance to City Hall\n",
    "geo_properties['dist_city_hall'] = geo_properties.distance(city_hall)\n",
    "\n",
    "#Convert from feet to miles\n",
    "geo_properties['dist_city_hall'] = geo_properties['dist_city_hall'] * 0.000189394"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance to nearest public transportation stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import public transportation shapefiles\n",
    "\n",
    "##Trolley stops (within Philadelphia county)\n",
    "trolley = gpd.read_file('https://services2.arcgis.com/9U43PSoL47wawX5S/arcgis/rest/services/Trolley_Stops1/FeatureServer/0/query?where=1%3D1&outFields=Route&outSR=4326&f=json')\n",
    "\n",
    "##MFL stops\n",
    "MFL = gpd.read_file('https://services2.arcgis.com/9U43PSoL47wawX5S/arcgis/rest/services/Market_Frankford_Line_Stations/FeatureServer/0/query?where=1%3D1&outFields=Route&outSR=4326&f=json')\n",
    "\n",
    "##BSL stops\n",
    "BSL = gpd.read_file('https://services2.arcgis.com/9U43PSoL47wawX5S/arcgis/rest/services/Broad_Street_Line_Stations/FeatureServer/0/query?where=1%3D1&outFields=Route&outSR=4326&f=json')\n",
    "\n",
    "##Regional Rail stops (within Philadelphia County)\n",
    "RR = gpd.read_file('https://services2.arcgis.com/9U43PSoL47wawX5S/arcgis/rest/services/Regional_Rail_Stations/FeatureServer/0/query?where=1%3D1&outFields=Line_Name,County&outSR=4326&f=json')\n",
    "\n",
    "#Drop Regional Rail stations that aren't in Philadelphia\n",
    "RR.drop(index=RR[RR['County'] != 'Philadelphia'].index, inplace=True)\n",
    "RR.drop(columns='County', inplace=True)\n",
    "RR.rename(columns={'Line_Name': 'Route'}, inplace=True)\n",
    "\n",
    "##Concatenate transportation stops into one gdf of all transportation stops\n",
    "transport = pd.concat([trolley, MFL, BSL, RR])\n",
    "transport.to_crs('epsg:2272', inplace=True)\n",
    "\n",
    "#delete individual transport files to save memory\n",
    "del(trolley)\n",
    "del(MFL)\n",
    "del(BSL)\n",
    "del(RR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding distance of each property to closest transport stop using scipy spatial cKDTree (fastest method)\n",
    "def nearest_transport(props, transport):   \n",
    "    '''Constructs cKDTree of transport stops, then returns Series of distances to closest transport stop\n",
    "    for each property'''\n",
    "    \n",
    "    #Convert geometry columns to arrays\n",
    "    prop_array = np.array(list(zip(props.geometry.x, props.geometry.y)))\n",
    "    transport_array = np.array(list(zip(transport.geometry.x, transport.geometry.y)))\n",
    "    \n",
    "    #Construct cKDTree of transportation stops\n",
    "    tree = cKDTree(transport_array)\n",
    "    \n",
    "    #Query tree to find distance to closest transport stop\n",
    "    dist, idx = tree.query(prop_array,k=1) #dist is array of distances to closest point, idx is array of indices of closest points (not needed)\n",
    "    \n",
    "    #Since Easting and Northing are in feet, distances have to be converted to miles\n",
    "    f = lambda x: x * 0.000189394\n",
    "    dist_miles = f(dist) \n",
    "    \n",
    "    return pd.Series(dist_miles, name='dist_to_transport', index=props.index)\n",
    "\n",
    "geo_properties['dist_to_transport'] = nearest_transport(geo_properties, transport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neighbourhood attributes (change from 2000 to 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract Philadelphia from LTDB data (2000 Census)\n",
    "def get_phl_data(fname):\n",
    "    if fname == 'https://github.com/caranvr/DSSS-predicting-demolition/blob/main/data/LTDB_Std_2000_fullcount/LTDB_Std_2000_fullcount.csv?raw=true':\n",
    "        cols = ['TRTID10', 'tract', 'county', 'POP00', 'NHWHT00', 'NHBLK00', 'HISP00', 'HU00', 'OWN00', 'RENT00']\n",
    "    elif fname == 'https://github.com/caranvr/DSSS-predicting-demolition/blob/main/data/LTDB_Std_2000_Sample.csv?raw=true':\n",
    "        cols = ['TRTID10', 'county', 'AG25UP00', 'COL00', 'HINC00']\n",
    "    \n",
    "    full_df = pd.read_csv(fname, usecols=cols)\n",
    "    \n",
    "    df = full_df.loc[\n",
    "        full_df['county'] == 'Philadelphia County'\n",
    "    ].copy()\n",
    "    \n",
    "    del(full_df)\n",
    "    return df\n",
    "\n",
    "#Load in full count and sample data for 2000, adjusted to 2010 boundaries\n",
    "census00 = get_phl_data('https://github.com/caranvr/DSSS-predicting-demolition/blob/main/data/LTDB_Std_2000_fullcount/LTDB_Std_2000_fullcount.csv?raw=true')\n",
    "census00_sample = get_phl_data('https://github.com/caranvr/DSSS-predicting-demolition/blob/main/data/LTDB_Std_2000_Sample.csv?raw=true')\n",
    "\n",
    "phl00 = pd.merge(census00, census00_sample, on='TRTID10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(census00)\n",
    "del(census00_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "phl00.set_index('TRTID10', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Census data from API (ACS 5-year estimates from 2013 to 2018)\n",
    "key = 'c0c4631460ee7868c6c89b3e7d58e9941bd285d7' #API access key\n",
    "variables = ['B01001_001E', 'B03002_003E', 'B03002_004E', 'B03001_003E', 'B25003_001E', 'B25003_002E', 'B25003_003E', 'B15002_001E',\n",
    "            'B15003_022E', 'B15003_023E', 'B15003_024E', 'B15003_025E', 'B19013_001E']\n",
    "state = '42' #state code\n",
    "county = '101' #county code\n",
    "\n",
    "url = 'https://api.census.gov/data/2018/acs/acs5?get=' + \",\".join(variables) + '&for=tract:*&in=state:' + state + '&in=county:' + county + '&key=' + key\n",
    "phl18 = pd.read_json(url)\n",
    "\n",
    "#Column names are in first row - make sure column names are in the right place\n",
    "phl18.columns = phl18.iloc[0]\n",
    "phl18.drop(index=phl18.index[0], axis=0, inplace=True)\n",
    "\n",
    "#Convert numeric variables to ints\n",
    "for v in variables:\n",
    "    if v != 'B19013_001E': #all variables except for median income\n",
    "        phl18[v] = phl18[v].astype('float').astype('Int64')\n",
    "    else:\n",
    "        phl18[v] = phl18[v].astype('float')\n",
    "        \n",
    "#Create new index for joining 2000 and 2013-18 Census data\n",
    "phl18['TRTID10'] = (phl18['state'] + phl18['county'] + phl18['tract']).astype('int')\n",
    "phl18.set_index('TRTID10', inplace=True)\n",
    "\n",
    "#Drop non-residential Census tracts\n",
    "phl18.drop(index=phl18[phl18['B01001_001E'] == 0].index, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for nulls, since they aren't encoded as nans\n",
    "check_nulls = phl18[variables].lt(0).sum(axis=1)\n",
    "null_rows = check_nulls[check_nulls > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert nulls to actual nans so they don't mess up the calculations\n",
    "phl18['B19013_001E'] = phl18['B19013_001E'].replace(-666666666, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add up post-college education fields\n",
    "phl18['COL18'] = phl18['B15003_022E'] + phl18['B15003_023E'] + phl18['B15003_024E'] + phl18['B15003_025E']\n",
    "\n",
    "#Drop constituent post-college education fields\n",
    "phl18.drop(columns=['B15003_022E', 'B15003_023E', 'B15003_024E', 'B15003_025E'], inplace=True)\n",
    "\n",
    "#Join 2000 and 2018 Census variables\n",
    "phl_change = pd.merge(phl18, phl00, how='left', left_index=True, right_index=True)\n",
    "\n",
    "#Change household income from 2000 to 2018 dollars\n",
    "phl_change['HINC00'] = phl_change['HINC00'].astype('float') * 1.5382"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join Census tract data from 2018 to each property (load Census tract shapefile, then gpd sjoin)\n",
    "phl_tracts = gpd.read_file('https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson')\n",
    "\n",
    "phl_tracts.rename(columns={'GEOID10': 'TRTID10'}, inplace=True)\n",
    "phl_tracts['TRTID10'] = phl_tracts['TRTID10'].astype('int')\n",
    "phl_tracts.set_index('TRTID10', inplace=True)\n",
    "\n",
    "#Reproject to allow for a spatial join between geo_properties and tracts\n",
    "phl_tracts.to_crs('epsg:2272', inplace=True)\n",
    "\n",
    "phl_change_geo = pd.merge(phl_tracts, phl_change, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatially join properties to Census tracts (only including residential Census tracts)\n",
    "geo_properties = gpd.sjoin(geo_properties, phl_change_geo, op='within')\n",
    "\n",
    "drop_cols = ['OBJECTID', 'STATEFP10', 'COUNTYFP10', 'NAMELSAD10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10', 'AWATER10',\n",
    "            'INTPTLAT10', 'INTPTLON10', 'LOGRECNO', 'state', 'county', 'tract_x', 'county_x', 'tract_y', 'county_y']\n",
    "\n",
    "geo_properties.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_properties.to_csv('geo_props_final.csv', chunksize=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
